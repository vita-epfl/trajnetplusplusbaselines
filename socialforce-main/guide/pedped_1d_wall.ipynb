{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import socialforce\n",
    "\n",
    "_ = torch.manual_seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pedped-1d-wall)=\n",
    "# 1D Wall\n",
    "\n",
    "\n",
    "## Parametric\n",
    "\n",
    "This is an extension of the {ref}`pedped-1d` example to study the robustness\n",
    "of the inference process to potentials with steep gradients.\n",
    "We use a modified $V(b)$ potential that could be described as a \"soft wall\"\n",
    "at $b=\\sigma$. The amount of gradients is determined by the width $w$ of this \n",
    "wall:\n",
    "\\begin{align}\n",
    "    V(b) &= \\exp\\left(- \\frac{b - \\sigma}{w} \\right)\n",
    "\\end{align}\n",
    "with its two parameters $\\sigma$ and $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped_ped = socialforce.potentials.PedPedPotentialWall(w=0.1)\n",
    "with socialforce.show.canvas(ncols=2) as (ax1, ax2): \n",
    "    ax1.set_ylim(0.0, 3.0)\n",
    "    ax2.set_ylim(-3.0, 0.0)\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        ped_ped, ax1, ax2, \n",
    "        label=r'true', sigma_label=r'true $\\sigma$', color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "We generate a single {ref}`Circle scenario <scenarios>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle = socialforce.scenarios.Circle(ped_ped=ped_ped)\n",
    "scenario = circle.generate(1)\n",
    "true_experience = socialforce.Trainer.scenes_to_experience(scenario)\n",
    "\n",
    "with socialforce.show.track_canvas() as ax:\n",
    "    socialforce.show.states(ax, scenario[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synthetic path shows a non-smooth direction change of the orange \n",
    "pedestrian. This is an artifact stemming from the finite step size in\n",
    "the simulation of the dynamics. We could remove this artifact by increasing\n",
    "the oversampling in our simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "We infer the parameters of an MLP to approximate the 1D scalar \n",
    "function $\\textrm{SF}(b)$ above from synthetic observations.\n",
    "The `PedPedPotentialMLP` is a two-layer MLP with softplus activations:\n",
    "\\begin{align}\n",
    "    \\textrm{MLP}(b) &= \\textrm{Softplus} \\;\\; L_{1\\times5} \\;\\; \\textrm{Softplus} \\;\\; L_{5\\times1} \\;\\; b\n",
    "\\end{align}\n",
    "which is written in terms of linear and non-linear operators where\n",
    "the Softplus operator applies the softplus function on its input from the right\n",
    "and $L$ is a linear operator (a matrix) with the subscript indicating the \n",
    "$\\textrm{output features} \\times \\textrm{input features}$.\n",
    "This two-layer MLP with 5 hidden units has 10 parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = socialforce.potentials.PedPedPotentialMLP(hidden_units=8)\n",
    "initial_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We use a standard optimizer from PyTorch (SGD).\n",
    "You can specify a standard PyTorch loss function for the `Trainer` as well\n",
    "but here the default of a `torch.nn.L1Loss()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = socialforce.Simulator(ped_ped=V)\n",
    "opt = torch.optim.SGD(V.parameters(), lr=3.0)\n",
    "socialforce.Trainer(simulator, opt).loop(100, true_experience, log_interval=10)\n",
    "final_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "with socialforce.show.canvas(ncols=2) as (ax1, ax2):\n",
    "    ax1.set_ylim(0.0, 3.0)\n",
    "    ax2.set_ylim(-3.0, 0.0)\n",
    "\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        circle.ped_ped, ax1, ax2, \n",
    "        label=r'true $V_0 e^{-b/\\sigma}$', sigma_label=r'true $\\sigma$', color='gray')\n",
    "\n",
    "    V.load_state_dict(initial_state_dict)\n",
    "    socialforce.show.potential_1d(V, ax1, ax2, label=r'initial MLP($b$)', linestyle='dashed', color='C0')\n",
    "\n",
    "    V.load_state_dict(final_state_dict)\n",
    "    socialforce.show.potential_1d(V, ax1, ax2, label=r'MLP($b$)', color='C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the numerical challenges impact our ability to infer the \n",
    "parameters of this potential. When we reduce $w$ further, this discrepancy grows."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('venv3')",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "29864609ce42acb949f1cb2f5c54bbb80a5cac9b20d76f096c9b799bd2af5ed7"
   }
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
