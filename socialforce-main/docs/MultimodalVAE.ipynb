{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sven Kreiss](https://www.svenkreiss.com), April 16 2020\n",
    "\n",
    "# Pedestrian Path Prediction Problem\n",
    "\n",
    "Given: past $(x, y)$ coordinates of a pedestrian  \n",
    "Looking for: future $(x, y)$ coordinates\n",
    "\n",
    "Pedestrian path prediction is inherently a non-deterministic process because the pedestrian has free will and is always able to make free choices that cannot be deduced from external variables. The problem must be solved in a statistical framework where multiple future choices are possible.\n",
    "\n",
    "\n",
    "## Simplified Problem\n",
    "\n",
    "The pedestrian just moves in one dimension $x$ and is currently at $x=0$. In the training data, in 33% of all cases, the pedestrian goes to $x=-1$ and in the remaining cases goes to $x=1$. We are only looking at a single step.\n",
    "\n",
    "While the data is discrete, we want to investigate methods that generalize to arbitrary coordinates, so we will only investigate methods that regress to output locations in a continuous way (_i.e._, we are not going to look at discrete choices between -1 and 1).\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "We already know deterministic models won't work. \n",
    "\n",
    "The `Simple` model below is a \"simple\" extension to a deterministic feed-forward Neural Network with an additional randomly sampled input. The random input makes the model non-deterministic. However, we don't have a learning method that leverages the random input and so these models produce similar results as their deterministic equivalents.\n",
    "\n",
    "One way to train a posterior distribution that leverages random sampling is with VAEs which are explored below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(1, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        r = torch.randn((x.shape[0], 1))\n",
    "        x = torch.nn.functional.relu(self.fc1(r))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = Simple()\n",
    "model_init = copy.deepcopy(model.state_dict())\n",
    "\n",
    "x_train = torch.zeros((1000, 1), requires_grad=False)\n",
    "x_pred = torch.zeros((1000000, 1), requires_grad=False)\n",
    "target = torch.ones((1000, 1), requires_grad=False)\n",
    "target[::3] = -1.0\n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model.load_state_dict(model_init)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.nn.functional.l1_loss(model(x_train), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predicted_l1 = model(x_pred)\n",
    "print(predicted_l1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model.load_state_dict(model_init)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.nn.functional.mse_loss(model(x_train), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predicted_mse = model(x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(target.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, label='ground truth', color='black')\n",
    "ax.hist(predicted_l1.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='L1')\n",
    "ax.hist(predicted_mse.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='MSE')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    \"\"\"Based on the pytorch VAE example.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.fc1 = torch.nn.Linear(1, 20)\n",
    "        self.fc21 = torch.nn.Linear(20, 2)\n",
    "        self.fc22 = torch.nn.Linear(20, 2)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc3 = torch.nn.Linear(2, 20)\n",
    "        self.fc4 = torch.nn.Linear(20, 20)\n",
    "        self.fc5 = torch.nn.Linear(20, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.nn.functional.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.nn.functional.relu(self.fc3(z))\n",
    "        h4 = torch.nn.functional.relu(self.fc4(h3))\n",
    "        return self.fc5(h4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mu, logvar = self.encode(x)\n",
    "        else:\n",
    "            mu = torch.zeros((x.shape[0], 2), requires_grad=False)\n",
    "            logvar = torch.zeros((x.shape[0], 2), requires_grad=False)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "vae_model = VAE()\n",
    "vae_model_init = copy.deepcopy(vae_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_mse(y_hat, target, mu, logvar, *, kld_prefactor=1.0):\n",
    "    recon_loss = torch.nn.functional.mse_loss(y_hat, target, reduction='mean')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / y_hat.shape[0]\n",
    "\n",
    "    return recon_loss + kld_prefactor * KLD\n",
    "\n",
    "def vae_loss_l1(y_hat, target, mu, logvar, *, kld_prefactor=1.0):\n",
    "    recon_loss = torch.nn.functional.l1_loss(y_hat, target, reduction='mean')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / y_hat.shape[0]\n",
    "\n",
    "    return recon_loss + kld_prefactor * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.load_state_dict(vae_model_init)\n",
    "vae_model.train()\n",
    "optimizer = torch.optim.SGD(vae_model.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, mu, logvar = vae_model(target)\n",
    "    loss = vae_loss_mse(y_hat, target, mu, logvar, kld_prefactor=2.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "predicted_vae, _, __ = vae_model(x_pred)\n",
    "print(predicted_vae[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_lowkl = VAE()\n",
    "vae_model_lowkl.load_state_dict(vae_model_init)\n",
    "vae_model_lowkl.train()\n",
    "optimizer = torch.optim.SGD(vae_model_lowkl.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, mu, logvar = vae_model_lowkl(target)\n",
    "    loss = vae_loss_mse(y_hat, target, mu, logvar, kld_prefactor=0.1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "vae_model_lowkl.eval()\n",
    "predicted_vae_lowkl, _, __ = vae_model_lowkl(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_l1 = VAE()\n",
    "vae_model_l1.load_state_dict(vae_model_init)\n",
    "vae_model_l1.train()\n",
    "optimizer = torch.optim.SGD(vae_model_l1.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, mu, logvar = vae_model_l1(target)\n",
    "    loss = vae_loss_l1(y_hat, target, mu, logvar, kld_prefactor=2.1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "vae_model_l1.eval()\n",
    "predicted_vae_l1, _, __ = vae_model_l1(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_lowkl_l1 = VAE()\n",
    "vae_model_lowkl_l1.load_state_dict(vae_model_init)\n",
    "vae_model_lowkl_l1.train()\n",
    "optimizer = torch.optim.SGD(vae_model_lowkl_l1.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, mu, logvar = vae_model_lowkl_l1(target)\n",
    "    loss = vae_loss_l1(y_hat, target, mu, logvar, kld_prefactor=0.1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "vae_model_lowkl_l1.eval()\n",
    "predicted_vae_lowkl_l1, _, __ = vae_model_lowkl_l1(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1.hist(target.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, label='ground truth', color='black')\n",
    "ax1.hist(predicted_l1.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='L1')\n",
    "ax1.hist(predicted_mse.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='MSE')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(target.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, label='ground truth', color='black')\n",
    "ax2.hist(predicted_vae.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='VAE')\n",
    "ax2.hist(predicted_vae_lowkl.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='VAE (low KL div)')\n",
    "ax2.hist(predicted_vae_l1.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='VAE ($L_1$ reconstruction)')\n",
    "ax2.hist(predicted_vae_lowkl_l1.detach().numpy(), bins=50, range=(-1.2, 1.2), density=True, alpha=0.8, label='VAE (low KL div, $L_1$ recon)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "__Reconstruction loss__: MSE tends to produce results in between the two modes. L1 collapses onto the dominant mode.\n",
    "\n",
    "__KL strength in VAE models__: VAE models reproduce the full posterior distribution unless the KL strength is too large in which case the models revert to their deterministic behavior (averaging for MSE, collapse for L1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).\n",
    "\n",
    "Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in neural information processing systems. 2015.\n",
    "\n",
    "Yan, Xinchen, et al. \"Mt-vae: Learning motion transformations to generate multimodal human dynamics.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
