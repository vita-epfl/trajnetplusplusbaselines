{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import socialforce\n",
    "\n",
    "_ = torch.manual_seed(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pedped-1d-bayesian)=\n",
    "# 1D Bayesian\n",
    "\n",
    "This is a Bayesian version of {ref}`pedped-1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = socialforce.potentials.PedPedPotential(sigma=0.3)\n",
    "V2 = socialforce.potentials.PedPedPotential(sigma=0.5)\n",
    "with socialforce.show.canvas(figsize=(12, 8), ncols=2, nrows=2) as ((ax1, ax2), (ax3, ax4)):\n",
    "    socialforce.show.potential_2d(V1, ax1)\n",
    "    socialforce.show.potential_2d_grad(V1, ax2)\n",
    "\n",
    "    socialforce.show.potential_2d(V2, ax3)\n",
    "    socialforce.show.potential_2d_grad(V2, ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pedestrian is located in the left focal point of the ellipse and at their \n",
    "current speed can reach the right focal point within one step that is assumed\n",
    "to take $\\Delta t = 0.4s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Scenario\n",
    "\n",
    "We generate 10 {ref}`Circle scenario <scenarios>`, five for each configuration\n",
    "of the potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario1 = socialforce.scenarios.Circle(ped_ped=V1).generate(5)\n",
    "scenario2 = socialforce.scenarios.Circle(ped_ped=V2).generate(5)\n",
    "true_experience = socialforce.Trainer.scenes_to_experience(scenario1 + scenario2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "with socialforce.show.track_canvas(figsize=(8, 5), ncols=2) as (ax1, ax2):\n",
    "    socialforce.show.states(ax1, scenario1[0], zorder=10)\n",
    "    for scenario in scenario1[1:]:\n",
    "        socialforce.show.states(ax1, scenario, alpha=0.5)\n",
    "\n",
    "    socialforce.show.states(ax2, scenario2[0], zorder=10)\n",
    "    for scenario in scenario2[1:]:\n",
    "        socialforce.show.states(ax2, scenario, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = socialforce.potentials.PedPedPotentialMLP()\n",
    "initial_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "simulator = socialforce.Simulator(ped_ped=V) \n",
    "opt = torch.optim.SGD(V.parameters(), lr=1.0)\n",
    "socialforce.Trainer(simulator, opt).loop(10, true_experience)\n",
    "final_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "with socialforce.show.canvas(ncols=2) as (ax1, ax2):\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        V1, ax1, ax2, \n",
    "        label=r'true $V_1 = V_0 e^{-b/\\sigma}$', sigma_label=r'true $\\sigma$', color='gray')\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        V2, ax1, ax2, \n",
    "        label=r'true $V_2 = V_0 e^{-b/\\sigma}$', sigma_label=r'true $\\sigma$', color='darkgray')\n",
    "\n",
    "    V.load_state_dict(initial_state_dict)\n",
    "    socialforce.show.potential_1d(V, ax1, ax2, label=r'initial MLP($b$)', linestyle='dashed', color='C0')\n",
    "\n",
    "    V.load_state_dict(final_state_dict)\n",
    "    socialforce.show.potential_1d(V, ax1, ax2, label=r'MLP($b$)', color='C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = socialforce.potentials.PedPedPotentialMLP(hidden_units=32, dropout_p=0.1)\n",
    "initial_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "simulator = socialforce.Simulator(ped_ped=V) \n",
    "opt = torch.optim.SGD(V.parameters(), lr=0.2)\n",
    "socialforce.Trainer(simulator, opt).loop(25, true_experience)\n",
    "final_state_dict = copy.deepcopy(V.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "with socialforce.show.canvas(ncols=2) as (ax1, ax2):\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        V1, ax1, ax2, \n",
    "        label=r'true $V_1 = V_0 e^{-b/\\sigma}$', sigma_label=r'true $\\sigma$', color='gray')\n",
    "    socialforce.show.potential_1d_parametric(\n",
    "        V2, ax1, ax2, \n",
    "        label=r'true $V_2 = V_0 e^{-b/\\sigma}$', sigma_label=r'true $\\sigma$', color='darkgray')\n",
    "\n",
    "    # V.load_state_dict(initial_state_dict)\n",
    "    # socialforce.show.potential_1d(V, ax1, ax2, label=r'initial MLP($b$)', linestyle='dashed', color='C0')\n",
    "\n",
    "    V.load_state_dict(final_state_dict)\n",
    "    socialforce.show.potential_1d(V, ax1, ax2, label=r'MLP($b$)', color='C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    \"\"\"Based on the pytorch VAE example.\"\"\"\n",
    "    \n",
    "    def __init__(self, *, predict_dim=1, hidden_units=20, z_dim=2):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # encoder\n",
    "        self.fc1 = torch.nn.Linear(predict_dim, hidden_units)\n",
    "        self.fc21 = torch.nn.Linear(hidden_units, z_dim)\n",
    "        self.fc22 = torch.nn.Linear(hidden_units, z_dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc3 = torch.nn.Linear(z_dim, hidden_units)\n",
    "        self.fc4 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc5 = torch.nn.Linear(hidden_units, predict_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.nn.functional.softplus(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.nn.functional.softplus(self.fc3(z))\n",
    "        h4 = torch.nn.functional.softplus(self.fc4(h3))\n",
    "        return self.fc5(h4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mu, logvar = self.encode(x)\n",
    "        else:\n",
    "            mu = torch.zeros((x.shape[0], self.z_dim), requires_grad=False)\n",
    "            logvar = torch.zeros((x.shape[0], self.z_dim), requires_grad=False)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "vae_model = VAE()\n",
    "vae_model_init = copy.deepcopy(vae_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_l1(y_hat, target, mu, logvar, *, kld_prefactor=1.0):\n",
    "    recon_loss = torch.nn.functional.l1_loss(y_hat, target, reduction='mean')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / y_hat.shape[0]\n",
    "\n",
    "    return recon_loss + kld_prefactor * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.load_state_dict(vae_model_init)\n",
    "vae_model.train()\n",
    "optimizer = torch.optim.SGD(vae_model.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, mu, logvar = vae_model(target)\n",
    "    loss = vae_loss_l1(y_hat, target, mu, logvar, kld_prefactor=2.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "29864609ce42acb949f1cb2f5c54bbb80a5cac9b20d76f096c9b799bd2af5ed7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv3': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "29864609ce42acb949f1cb2f5c54bbb80a5cac9b20d76f096c9b799bd2af5ed7"
   }
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
